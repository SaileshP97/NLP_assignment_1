{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set up the WebDriver (Make sure you have ChromeDriver installed and in your PATH)\n",
    "\n",
    "# URL of the page\n",
    "# url = \"https://www.gujaratilexicon.com/gujarati-blogs/gujarati-language/international-language-day-and-ratilal-chandaria/\"\n",
    "# url=\"https://www.gujaratilexicon.com/gujarati-blogs/gujarati-language/tips-to-improve-handwriting-in-gujarati-language/\"\n",
    "# url=\"https://www.gujaratilexicon.com/gujarati-blogs/gujarati-language/importance-of-punctuation-marks-in-gujarati-language/\"\n",
    "# url=\"https://www.gujaratilexicon.com/gujarati-blogs/events-across-gujarat/about-matrubhasha-abhiyan-and-its-activities/\"\n",
    "# Load the webpage\n",
    "def fun(url,i):\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow some time for the page to fully load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Initialize empty lists to hold the data\n",
    "    titles = []\n",
    "    dates = []\n",
    "    paragraphs_data = []\n",
    "\n",
    "    try:\n",
    "        # Extract the title (assuming it's in an <h1> tag)\n",
    "        title_element = driver.find_element(By.TAG_NAME, 'h1')\n",
    "        title = title_element.text\n",
    "        titles.append(title)\n",
    "\n",
    "        # Extract the date (assuming it's in a <time> or <span> tag, depending on the structure)\n",
    "        try:\n",
    "            date_element = driver.find_element(By.TAG_NAME, 'time')  # Change to appropriate tag if needed\n",
    "            date = date_element.text\n",
    "        except:\n",
    "            date_element = driver.find_element(By.CLASS_NAME, 'info.pb-3.border-bottom')  # Or an appropriate class for date\n",
    "            date = date_element.text\n",
    "        dates.append(date)\n",
    "\n",
    "        # Extract the main content (assuming it's inside a div with the given class)\n",
    "        content_div = driver.find_element(By.CLASS_NAME, 'info.pb-3.border-bottom')\n",
    "        paragraphs = content_div.find_elements(By.TAG_NAME, 'p')\n",
    "\n",
    "        # Loop through and append each paragraph's text to the list\n",
    "        for p in paragraphs:\n",
    "            paragraphs_data.append(p.text)\n",
    "\n",
    "    finally:\n",
    "        # Close the driver after scraping\n",
    "        driver.quit()\n",
    "\n",
    "    # Create a DataFrame with multiple columns\n",
    "    df = pd.DataFrame({\n",
    "        \"Title\": titles * len(paragraphs_data),  # Repeat the title for each paragraph\n",
    "        \"Date\": dates * len(paragraphs_data),    # Repeat the date for each paragraph\n",
    "        \"Paragraph\": paragraphs_data             # Paragraphs as separate rows\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    # print(df)\n",
    "\n",
    "    # Save the DataFrame to a CSV file (optional)\n",
    "    df.to_csv(f\"extracted_blog_data{i}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add links to  below list in order to extract data \n",
    "url=[\"\",\n",
    "     \"\",\n",
    "     \"\",\n",
    "     \"\",\n",
    "       ]\n",
    "\n",
    "\n",
    "\n",
    "i=15\n",
    "# fun(url[0],i)\n",
    "for u in url:\n",
    "    try:\n",
    "        fun(u,i)\n",
    "    except:\n",
    "        continue\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def text_files_to_csv(folder_path, output_csv):\n",
    "    # List to store file data\n",
    "    data = []\n",
    "    num=1\n",
    "    # Iterate through all the files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()  # Read and strip any trailing newlines\n",
    "                data.append([f\"Serial number {num} :\", content])  # Append file name and content as a row\n",
    "                num+=1\n",
    "\n",
    "    # Write data to a CSV file\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['File Name', 'Content'])  # Write the header row\n",
    "        writer.writerows(data)  # Write all rows\n",
    "\n",
    "    # print(f\"All text files have been successfully converted into '{output_csv}'.\")\n",
    "    # pd.to_\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# folder_path = 'E:/NLP/Assignments/Gujarati_Artical_Dataset/Business/BBC business'  # Replace with the folder path\n",
    "# # output_csv = 'bbc_business.csv'  # Replace with the desired output CSV file name\n",
    "# folder_path=\"E:/NLP/Assignments/Gujarati_Artical_Dataset\"\n",
    "# for folder_name in os.listdir(folder_path):\n",
    "#     for sub_folder_name in os.listdir(f\"{folder_path}/{folder_name}\"):\n",
    "#         try:\n",
    "#             text_files_to_csv(f\"{folder_path}/{folder_name}/{sub_folder_name}\", f\"{sub_folder_name}.csv\")\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "\n",
    "# text_files_to_csv(\"E:/NLP/fake and real news data/Gujarati_fake_news\",\"fake_news.csv\")\n",
    "text_files_to_csv(\"E:/NLP/fake and real news data/Gujarati_real_news\",\"real_news.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframe with two cols (source link of data, size of data taken from it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of data is in MBs\n",
    "dict={\"https://www.gujaratilexicon.com\":\"5\",\n",
    "      \"https://gujarati.news18.com/\":\"1\",\n",
    "      \"https://www.newspremi.com/\":\"1\",\n",
    "      \"https://huggingface.co/\":\"200\",\n",
    "      \"https://www.bbc.com/news\":\"78 \",\n",
    "      \"https://chitralekha.com/\":\"174\",\n",
    "      \"https://gujaratexclusive.in/\":\"7\",\n",
    "      \"https://www.gujaratpost.in/\":\"105\",\n",
    "      \"https://www.gujarattak.in/\":\"120\",\n",
    "      \"https://epaper.gujarattimesusa.com/\":\"35\",\n",
    "      \"https://indianexpress.com/\":\"235\",\n",
    "      \"https://www.kaltak24.com/\":\"7\",\n",
    "      \"https://tv9gujarati.com/live-tv\":\"20\",\n",
    "      \"https://www.magicbricks.com/\":\"40\",\n",
    "      \"https://mantavyanews.com/\":\"400\",\n",
    "      \"https://morbiupdate.com/\":\"320\",\n",
    "      \"https://www.navajivan.in/\":\"90\",\n",
    "      \"https://nbs.net/\":\"15\",\n",
    "      \"https://www.opindia.com/category/international/\":\"283\",\n",
    "      \"https://theeconomicbusiness.com/\":\"78\",\n",
    "      \"https://trishulnews.com/\":\"231\",\n",
    "      \"https://www.westerntimes.co.in/\":\"107\",\n",
    "      \"https://www.kaggle.com/\":\"118\"\n",
    "      }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
