{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests\n",
    "# %pip install langdetect\n",
    "# %pip install beautifulsoup4 lxml \n",
    "# %pip install scrapy\n",
    "# %pip install hashlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "MAX_PAGES = 20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_gujarati(text):\n",
    "    \"\"\"Check if the text is in Gujarati using language detection.\"\"\"\n",
    "    try:\n",
    "        return detect(text) == 'gu'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def crawl_page(url):\n",
    "    \"\"\"Crawl a single webpage and return Gujarati text.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to retrieve {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    paragraphs = soup.find_all(['p', 'div'])\n",
    "    \n",
    "    gujarati_texts = []\n",
    "    for para in paragraphs:\n",
    "        text = para.get_text(strip=True)\n",
    "        if is_gujarati(text):\n",
    "            gujarati_texts.append(text)\n",
    "    \n",
    "    return gujarati_texts\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Save extracted data to a CSV file.\"\"\"\n",
    "    with open(filename, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # writer.writerow([\"Content\"])\n",
    "        for row in data:\n",
    "            writer.writerow([row])\n",
    "\n",
    "def crawl_multiple_pages(start_url, max_pages=MAX_PAGES, delay=1):\n",
    "    \"\"\"Crawl multiple pages starting from the given URL, handle pagination if needed.\"\"\"\n",
    "    all_gujarati_texts = []\n",
    "    current_url = start_url\n",
    "    for i in range(max_pages):\n",
    "        print(f\"Crawling page {i+1}: {current_url}\")\n",
    "        page_texts = crawl_page(current_url)\n",
    "        all_gujarati_texts.extend(page_texts)\n",
    "\n",
    "        # For this example, we just simulate multiple pages by appending `page=i` to URL.\n",
    "        # You may need to modify this based on the website's pagination structure.\n",
    "        next_page_url = current_url + f\"?page={i+1}\"\n",
    "        current_url = next_page_url\n",
    "        \n",
    "        # Avoid overloading the server\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return all_gujarati_texts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.divyabhaskar.co.in/\"\n",
    "\n",
    "    gujarati_content = crawl_multiple_pages(start_url, max_pages=5)\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    if gujarati_content:\n",
    "        save_to_csv(gujarati_content, \"gujarati_content.csv\")\n",
    "        print(f\"Saved {len(gujarati_content)} lines of Gujarati content to 'gujarati_content.csv'.\")\n",
    "    else:\n",
    "        print(\"No Gujarati content found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "import time\n",
    "\n",
    "def is_gujarati(text):\n",
    "    \"\"\"Check if the text is in Gujarati using language detection.\"\"\"\n",
    "    try:\n",
    "        return detect(text) == 'gu'\n",
    "    except:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_page(url):\n",
    "    \"\"\"Extract all links from the specified structure in the webpage.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to retrieve {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    # Find the main div that contains the ul tag\n",
    "    main_div = soup.find('div', class_='ba1e62a6')  # Modify class to target the correct div\n",
    "    if not main_div:\n",
    "        print(f\"No div found with the specified structure on {url}\")\n",
    "        return []\n",
    "\n",
    "    # Extract the ul tag\n",
    "    ul_tag = main_div.find('ul')\n",
    "    if not ul_tag:\n",
    "        print(f\"No ul tag found in the div on {url}\")\n",
    "        return []\n",
    "\n",
    "    links = []\n",
    "    # Find all div elements under the ul tag\n",
    "    div_elements = ul_tag.find_all('div')\n",
    "\n",
    "    for div in div_elements:\n",
    "        # Inside each div, find the <a> tag\n",
    "        a_tag = div.find('a', href=True)\n",
    "        if a_tag:\n",
    "            link = a_tag['href']\n",
    "            links.append(link)\n",
    "\n",
    "    return links\n",
    "\n",
    "def crawl_page(url):\n",
    "    \"\"\"Crawl a single webpage and return Gujarati text.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to retrieve {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    paragraphs = soup.find_all(['p', 'div'])\n",
    "\n",
    "    gujarati_texts = []\n",
    "    for para in paragraphs:\n",
    "        text = para.get_text(strip=True)\n",
    "        if is_gujarati(text):\n",
    "            gujarati_texts.append(text)\n",
    "\n",
    "    return gujarati_texts\n",
    "\n",
    "def crawl_links_and_extract_text(start_url, delay=1):\n",
    "    \"\"\"Crawl all links from a page and extract Gujarati text from each link.\"\"\"\n",
    "    all_gujarati_texts = []\n",
    "\n",
    "    # Step 1: Get all the links from the main page\n",
    "    links = get_links_from_page(start_url)\n",
    "    if not links:\n",
    "        print(f\"No links found on {start_url}\")\n",
    "        return\n",
    "    else:\n",
    "        print(len(links))\n",
    "\n",
    "    # Step 2: Visit each link and extract Gujarati text\n",
    "    for i, link in enumerate(links):\n",
    "        print(f\"Crawling link {i+1}/{len(links)}: {link}\")\n",
    "        gujarati_texts = crawl_page(start_url+link)\n",
    "        all_gujarati_texts.extend(gujarati_texts)\n",
    "\n",
    "        # Avoid overloading the server\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return all_gujarati_texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.divyabhaskar.co.in/\"  # Replace with your URL\n",
    "    gujarati_content = crawl_links_and_extract_text(start_url)\n",
    "\n",
    "    # Optionally, print the extracted Gujarati content\n",
    "    if gujarati_content:\n",
    "        for content in gujarati_content:\n",
    "            print(content)\n",
    "    else:\n",
    "        print(\"No Gujarati content found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling link: /international/news/for-the-first-time-in-human-history-spacewalk-by-two-astronauts-700-km-above-earth-133632824.html\n",
      "Saved 32 lines of Gujarati content to 'gujarati_contents.csv'.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "class GujaratiSpider(scrapy.Spider):\n",
    "    name = 'gujarati_spider'\n",
    "    \n",
    "    # Seed URLs to start with\n",
    "    start_urls = [\n",
    "        'https://www.divyabhaskar.co.in/',  # Replace with the actual dynamic Gujarati language website\n",
    "    ]\n",
    "    \n",
    "    # Track visited URLs to avoid revisiting the same links\n",
    "    visited_urls = set()\n",
    "\n",
    "    # Regular expression pattern to match Gujarati text\n",
    "    gujarati_pattern = re.compile(r'[\\u0A80-\\u0AFF]+')\n",
    "\n",
    "    # Directory to save each crawled page's content\n",
    "    output_directory = 'crawled_data'\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield scrapy.Request(url, callback=self.parse_dynamic_content)\n",
    "\n",
    "    def parse_dynamic_content(self, response):\n",
    "        # Extract text from the initial loaded page\n",
    "        yield from self.extract_and_save_text(response)\n",
    "\n",
    "        # Simulate scrolling down to load more content if needed\n",
    "        # You need to implement this part based on the specific behavior of the website\n",
    "        # For example, you might need to use JavaScript to scroll down or analyze XHR requests\n",
    "\n",
    "        # Here, we assume that the website loads more content dynamically through scrolling\n",
    "        # Replace this logic with actual code to handle dynamic loading\n",
    "        # For demonstration, we just yield the initial response again\n",
    "        yield response\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract and save text from the loaded content\n",
    "        yield from self.extract_and_save_text(response)\n",
    "\n",
    "        # Get all unique links on the page\n",
    "        links = response.xpath('//a/@href').getall()\n",
    "        unique_links = set()\n",
    "        \n",
    "        for link in links:\n",
    "            # Process only valid URLs that match the allowed domain prefixes\n",
    "            parsed_link = urlparse(link)\n",
    "            link_domain = parsed_link.netloc\n",
    "            \n",
    "            if link.startswith('http') and link_domain in self.allowed_domains and link not in self.visited_urls:\n",
    "                unique_links.add(link)\n",
    "                self.visited_urls.add(link)  # Add to visited set\n",
    "\n",
    "        # Crawl each unique link that has the same domain prefix\n",
    "        for unique_link in unique_links:\n",
    "            yield response.follow(unique_link, self.parse)\n",
    "\n",
    "    def extract_and_save_text(self, response):\n",
    "        # Extract text from paragraphs and divs\n",
    "        page_text = response.xpath('//p/text()').getall()\n",
    "        page_text += response.xpath('//div/text()').getall()\n",
    "        page_text = ' '.join(page_text).strip()\n",
    "        \n",
    "        # Filter only Gujarati text\n",
    "        gujarati_text = ' '.join(self.gujarati_pattern.findall(page_text))\n",
    "        \n",
    "        # Save the Gujarati content to a new text file\n",
    "        if gujarati_text:\n",
    "            # Create output directory if it doesn't exist\n",
    "            if not os.path.exists(self.output_directory):\n",
    "                os.makedirs(self.output_directory)\n",
    "\n",
    "            # Generate a unique filename based on the URL or use sequential numbering\n",
    "            url_hash = hashlib.md5(response.url.encode()).hexdigest()\n",
    "            filename = os.path.join(self.output_directory, f'{url_hash}.txt')\n",
    "\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(gujarati_text + '\\n')\n",
    "        \n",
    "        # Return the extracted text for further processing if needed\n",
    "        return gujarati_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Regular expression to match Gujarati text\n",
    "gujarati_pattern = re.compile(r'[\\u0A80-\\u0AFF]+')\n",
    "\n",
    "# Directory to save output files\n",
    "output_directory = 'crawled_data'\n",
    "\n",
    "# Function to initialize the WebDriver (Chrome in headless mode)\n",
    "def init_driver():\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "    except:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        print(\"Running in headless mode.\")\n",
    "    \n",
    "    # chrome_options = Options()\n",
    "    # chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "    # chrome_options.add_argument(\"--no-sandbox\")\n",
    "    # chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # # Provide the path to your ChromeDriver\n",
    "    # driver = webdriver.Chrome(executable_path='/path/to/chromedriver', options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "# Function to scroll and load more content dynamically\n",
    "def scroll_down(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        # Scroll down to the bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        # Wait for new content to load\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if new_height == last_height:\n",
    "            # If the height hasn't changed, stop scrolling\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "# Function to extract Gujarati text from a webpage\n",
    "def extract_gujarati_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract all text\n",
    "    page_text = soup.get_text(separator=' ')\n",
    "    \n",
    "    # Filter out only Gujarati text\n",
    "    gujarati_text = ' '.join(gujarati_pattern.findall(page_text))\n",
    "    \n",
    "    return gujarati_text\n",
    "\n",
    "# Function to save the extracted text to a file\n",
    "def save_text_to_file(url, text):\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # Generate a unique filename based on the URL\n",
    "    url_hash = hashlib.md5(url.encode()).hexdigest()\n",
    "    filename = os.path.join(output_directory, f'{url_hash}.txt')\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "# Main function to crawl a dynamic webpage\n",
    "def crawl_dynamic_website(url):\n",
    "    driver = init_driver()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll down to load all dynamic content\n",
    "    scroll_down(driver)\n",
    "    \n",
    "    # Get the page source after all content is loaded\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Extract Gujarati text\n",
    "    gujarati_text = extract_gujarati_text(page_source)\n",
    "    \n",
    "    # Save the text to a file\n",
    "    if gujarati_text:\n",
    "        save_text_to_file(url, gujarati_text)\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = 'https://www.divyabhaskar.co.in/'  # Replace with the actual dynamic website URL\n",
    "crawl_dynamic_website(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
