{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Regular expression to match Gujarati text\n",
    "gujarati_pattern = re.compile(r'[\\u0A80-\\u0AFF]+')\n",
    "\n",
    "# Directory to save output files\n",
    "output_directory = 'crawled_data'\n",
    "\n",
    "# Function to initialize the WebDriver (Chrome in headless mode)\n",
    "def init_driver():\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "    except:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        print(\"Running in headless mode.\")\n",
    "    return driver\n",
    "\n",
    "# Function to scroll and load more content dynamically\n",
    "def scroll_down(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        # Scroll down to the bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        # Wait for new content to load\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if new_height == last_height:\n",
    "            # If the height hasn't changed, stop scrolling\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "# Function to extract Gujarati text from a webpage\n",
    "def extract_gujarati_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract all text\n",
    "    page_text = soup.get_text(separator=' ')\n",
    "    \n",
    "    # Filter out only Gujarati text\n",
    "    gujarati_text = ' '.join(gujarati_pattern.findall(page_text))\n",
    "    \n",
    "    return gujarati_text\n",
    "\n",
    "# Function to save the extracted text to a file\n",
    "def save_text_to_file(url, text):\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # Generate a unique filename based on the URL\n",
    "    url_hash = hashlib.md5(url.encode()).hexdigest()\n",
    "    filename = os.path.join(output_directory, f'{url_hash}.txt')\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to crawl a dynamic webpage\n",
    "def crawl_dynamic_website(url):\n",
    "    driver = init_driver()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll down to load all dynamic content\n",
    "    scroll_down(driver)\n",
    "    \n",
    "    # Get the page source after all content is loaded\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Extract Gujarati text\n",
    "    gujarati_text = extract_gujarati_text(page_source)\n",
    "    \n",
    "    # Save the text to a file\n",
    "    if gujarati_text:\n",
    "        save_text_to_file(url, gujarati_text)\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to crawl a dynamic webpage\n",
    "def crawl_dynamic_website(url):\n",
    "    driver = init_driver()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll down to load all dynamic content\n",
    "    scroll_down(driver)\n",
    "    \n",
    "    # Get the page source after all content is loaded\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Extract Gujarati text\n",
    "    gujarati_text = extract_gujarati_text(page_source)\n",
    "    \n",
    "    # Save the text to a file\n",
    "    if gujarati_text:\n",
    "        save_text_to_file(url, gujarati_text)\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Function to extract links and Gujarati text from a specific div tag, and recursively visit new links\n",
    "def crawl_div_and_extract(driver, url, base_url, div_xpath):\n",
    "    # Visit the webpage\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Get the current page's source after loading\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Extract the specific <div> content\n",
    "    target_div = soup.select_one(div_xpath)\n",
    "    \n",
    "    if target_div:\n",
    "        # Extract and save Gujarati text from the specific div\n",
    "        gujarati_text = extract_gujarati_text(str(target_div))\n",
    "        if gujarati_text:\n",
    "            save_text_to_file(url, gujarati_text)\n",
    "        \n",
    "        # Extract all new links within the div\n",
    "        new_links = [a['href'] for a in target_div.find_all('a', href=True)]\n",
    "        \n",
    "        # Normalize and filter new links to only include those from the same base URL\n",
    "        valid_links = []\n",
    "        for link in new_links:\n",
    "            # Normalize the link\n",
    "            normalized_link = urljoin(base_url, link)\n",
    "            parsed_link = urlparse(normalized_link)\n",
    "            \n",
    "            # Only allow links that share the same base domain\n",
    "            if parsed_link.netloc == urlparse(base_url).netloc and normalized_link not in visited_urls:\n",
    "                valid_links.append(normalized_link)\n",
    "                visited_urls.add(normalized_link)  # Mark the URL as visited\n",
    "        \n",
    "        # Recursively visit the valid links and extract data\n",
    "        for valid_link in valid_links:\n",
    "            crawl_div_and_extract(driver, valid_link, base_url, div_xpath)\n",
    "\n",
    "# Example: Main function to initiate the crawling\n",
    "def start_recursive_crawl(start_url, div_xpath):\n",
    "    driver = init_driver()  # Initialize Selenium WebDriver\n",
    "    base_url = urlparse(start_url).scheme + \"://\" + urlparse(start_url).netloc  # Get the base URL\n",
    "    \n",
    "    # Initialize visited URLs set\n",
    "    global visited_urls\n",
    "    visited_urls = set()  # Track visited URLs to avoid loops\n",
    "    \n",
    "    # Start crawling from the initial URL\n",
    "    crawl_div_and_extract(driver, start_url, base_url, div_xpath)\n",
    "    \n",
    "    # Close the driver when done\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.divyabhaskar.co.in/', 'https://sandesh.com/', 'https://www.gujaratsamachar.com/', 'https://www.akilanews.com/', 'https://navgujaratsamay.com/', 'https://gujaratmitra.in/']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('url.csv', header=None)\n",
    "# print(df)\n",
    "\n",
    "baseUrls = df[0].tolist()\n",
    "print(baseUrls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "url = baseUrls[0]\n",
    "print(type(baseUrls[0]))\n",
    "print(type(url[0]))\n",
    "# crawl_dynamic_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "start_url = baseUrls[0]  # Replace with the actual URL\n",
    "div_xpath = 'div.ba1e62a6'  # Replace with the XPath or CSS selector of the target div\n",
    "start_recursive_crawl(start_url, div_xpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
